{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNU4CnD8ubsVZIFLRvvPkX0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"6173cceb78434c63b42482c1b48d16bc":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3778731a62b2441ea6d3bcfeb92143a4","IPY_MODEL_b243e574fa144c29899febc7c26a714e","IPY_MODEL_d29f9fb94e494e5189537e36169f35c6"],"layout":"IPY_MODEL_70f3ab4eab8245e9b63d448d605e8b60"}},"3778731a62b2441ea6d3bcfeb92143a4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e7c2f4aacb4549a8beec160cfb772050","placeholder":"​","style":"IPY_MODEL_dd511358646a4c3e9a50fa895ccb4ff1","value":"Processing: 100%"}},"b243e574fa144c29899febc7c26a714e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_25f85667a1b14a72ab4c8546dc05fccc","max":3201,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c14c19a26ed548b8b4f06226787d3892","value":3201}},"d29f9fb94e494e5189537e36169f35c6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cd0dfe74c29e4a30af9d5a44866f1f25","placeholder":"​","style":"IPY_MODEL_26560b43416d4175b0fcdb4ffff951aa","value":" 3201/3201 [00:30&lt;00:00, 114.73it/s]"}},"70f3ab4eab8245e9b63d448d605e8b60":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e7c2f4aacb4549a8beec160cfb772050":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dd511358646a4c3e9a50fa895ccb4ff1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"25f85667a1b14a72ab4c8546dc05fccc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c14c19a26ed548b8b4f06226787d3892":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"cd0dfe74c29e4a30af9d5a44866f1f25":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"26560b43416d4175b0fcdb4ffff951aa":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["This notebook will convert every tweet into a 768-dimensional vector that captures its semantic meaning."],"metadata":{"id":"RHyq3yzTC0EN"}},{"cell_type":"markdown","source":["# Cell 1: Setup & Load Model"],"metadata":{"id":"xcktoe3CC1lG"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ftb6k-QVCqxq","executionInfo":{"status":"ok","timestamp":1765987467582,"user_tz":300,"elapsed":159081,"user":{"displayName":"Darren Deng","userId":"14952694693563686285"}},"outputId":"dfdaa531-5085-4024-8a72-48195d2e6260"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Using device: cuda\n","\n","Loading fine-tuned model from: /content/drive/MyDrive/same_words_different_worlds/models/fine_tuned_roberta\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of RobertaModel were not initialized from the model checkpoint at /content/drive/MyDrive/same_words_different_worlds/models/fine_tuned_roberta and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["✓ Model loaded successfully\n","  Hidden size: 768\n"]}],"source":["# Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Imports\n","import os\n","import pandas as pd\n","import numpy as np\n","import torch\n","from transformers import AutoTokenizer, AutoModel\n","from tqdm.auto import tqdm\n","\n","# Check GPU\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","\n","# Define paths\n","BASE_PATH = '/content/drive/MyDrive/same_words_different_worlds'\n","\n","PATHS = {\n","    'raw': os.path.join(BASE_PATH, 'data/raw'),\n","    'processed': os.path.join(BASE_PATH, 'data/processed'),\n","    'outputs': os.path.join(BASE_PATH, 'data/outputs'),\n","    'models': os.path.join(BASE_PATH, 'models'),\n","    'figures': os.path.join(BASE_PATH, 'figures'),\n","}\n","\n","# Load fine-tuned model and tokenizer\n","model_path = os.path.join(PATHS['models'], 'fine_tuned_roberta')\n","print(f\"\\nLoading fine-tuned model from: {model_path}\")\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_path)\n","model = AutoModel.from_pretrained(model_path).to(device)\n","model.eval()  # Set to evaluation mode\n","\n","print(f\"✓ Model loaded successfully\")\n","print(f\"  Hidden size: {model.config.hidden_size}\")"]},{"cell_type":"markdown","source":["# Cell 2: Load Data"],"metadata":{"id":"xN4XdwtGEBSi"}},{"cell_type":"code","source":["# Load cleaned data\n","df = pd.read_csv(os.path.join(PATHS['processed'], '01_ai_tweets_clean.csv'))\n","\n","# Ensure clean_text is string type\n","df['clean_text'] = df['clean_text'].fillna(\"\").astype(str)\n","\n","print(f\"Loaded {len(df):,} tweets\")\n","print(f\"Columns: {df.columns.tolist()}\")\n","print(f\"\\nParty distribution:\")\n","print(df['party'].value_counts())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cfDgFGjHEDk-","executionInfo":{"status":"ok","timestamp":1765987601810,"user_tz":300,"elapsed":1259,"user":{"displayName":"Darren Deng","userId":"14952694693563686285"}},"outputId":"af2e7508-592e-41c9-a400-501b431cdad0"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded 3,201 tweets\n","Columns: ['party', 'text', 'clean_text', 'posted_date', 'year', 'name', 'chamber']\n","\n","Party distribution:\n","party\n","D    2022\n","R    1179\n","Name: count, dtype: int64\n"]}]},{"cell_type":"markdown","source":["# Cell 3: Define Embedding Extraction Function"],"metadata":{"id":"ZBaqdWOMEKXC"}},{"cell_type":"code","source":["def get_embedding(text, tokenizer, model, device, max_length=128):\n","    \"\"\"\n","    Extract a 768-dimensional embedding for a text using mean pooling.\n","\n","    Mean pooling averages all token embeddings (excluding padding),\n","    giving a single vector representation of the entire text.\n","    \"\"\"\n","    if not isinstance(text, str) or len(text.strip()) == 0:\n","        return np.zeros(768)\n","\n","    # Tokenize\n","    inputs = tokenizer(\n","        text,\n","        return_tensors=\"pt\",\n","        truncation=True,\n","        padding=True,\n","        max_length=max_length\n","    ).to(device)\n","\n","    # Get model output (no gradient computation needed)\n","    with torch.no_grad():\n","        outputs = model(**inputs)\n","\n","    # outputs.last_hidden_state shape: (batch_size, seq_len, hidden_dim=768)\n","    hidden_states = outputs.last_hidden_state\n","\n","    # Mean pooling: average over sequence length, ignoring padding tokens\n","    attention_mask = inputs['attention_mask'].unsqueeze(-1)  # (1, seq_len, 1)\n","    attention_mask = attention_mask.expand(hidden_states.size()).float()\n","\n","    # Sum of embeddings weighted by attention mask\n","    sum_embeddings = torch.sum(hidden_states * attention_mask, dim=1)\n","\n","    # Number of non-padding tokens\n","    sum_mask = torch.clamp(attention_mask.sum(dim=1), min=1e-9)\n","\n","    # Mean embedding\n","    mean_embedding = sum_embeddings / sum_mask\n","\n","    return mean_embedding.cpu().numpy()[0]\n","\n","# Test the function\n","test_text = \"AI safety regulation is essential for protecting Americans.\"\n","test_embedding = get_embedding(test_text, tokenizer, model, device)\n","\n","print(\"✓ Embedding function defined\")\n","print(f\"\\nTest embedding:\")\n","print(f\"  Input: '{test_text}'\")\n","print(f\"  Output shape: {test_embedding.shape}\")\n","print(f\"  Sample values: [{test_embedding[0]:.4f}, {test_embedding[1]:.4f}, ... {test_embedding[-1]:.4f}]\")\n","print(f\"  Embedding norm: {np.linalg.norm(test_embedding):.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GH_LHdZvEMkm","executionInfo":{"status":"ok","timestamp":1765987642184,"user_tz":300,"elapsed":699,"user":{"displayName":"Darren Deng","userId":"14952694693563686285"}},"outputId":"b2b1c9ed-fa84-4c34-c3eb-ca08cfc9608c"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["✓ Embedding function defined\n","\n","Test embedding:\n","  Input: 'AI safety regulation is essential for protecting Americans.'\n","  Output shape: (768,)\n","  Sample values: [-0.0016, 0.1230, ... 0.0581]\n","  Embedding norm: 12.6817\n"]}]},{"cell_type":"markdown","source":["# Cell 4: Extract Embeddings for All Tweets"],"metadata":{"id":"t1dPUfw0EYvY"}},{"cell_type":"code","source":["# Extract embeddings for all tweets\n","# This will take a few minutes (~3-5 min for 3,201 tweets)\n","\n","print(\"Extracting embeddings for all tweets...\")\n","print(\"=\"*60)\n","\n","embeddings = []\n","\n","for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing\"):\n","    emb = get_embedding(row['clean_text'], tokenizer, model, device)\n","    embeddings.append(emb)\n","\n","# Add embeddings to dataframe\n","df['embedding'] = embeddings\n","\n","print(\"=\"*60)\n","print(f\"✓ Extraction complete!\")\n","print(f\"  Total embeddings: {len(embeddings)}\")\n","print(f\"  Embedding shape: {df['embedding'].iloc[0].shape}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":160,"referenced_widgets":["6173cceb78434c63b42482c1b48d16bc","3778731a62b2441ea6d3bcfeb92143a4","b243e574fa144c29899febc7c26a714e","d29f9fb94e494e5189537e36169f35c6","70f3ab4eab8245e9b63d448d605e8b60","e7c2f4aacb4549a8beec160cfb772050","dd511358646a4c3e9a50fa895ccb4ff1","25f85667a1b14a72ab4c8546dc05fccc","c14c19a26ed548b8b4f06226787d3892","cd0dfe74c29e4a30af9d5a44866f1f25","26560b43416d4175b0fcdb4ffff951aa"]},"id":"X8hyLdGcEZzr","executionInfo":{"status":"ok","timestamp":1765987726078,"user_tz":300,"elapsed":30038,"user":{"displayName":"Darren Deng","userId":"14952694693563686285"}},"outputId":"19fc929a-ef67-4b75-cc78-adeb1177b554"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Extracting embeddings for all tweets...\n","============================================================\n"]},{"output_type":"display_data","data":{"text/plain":["Processing:   0%|          | 0/3201 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6173cceb78434c63b42482c1b48d16bc"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["============================================================\n","✓ Extraction complete!\n","  Total embeddings: 3201\n","  Embedding shape: (768,)\n"]}]},{"cell_type":"markdown","source":["# Cell 5: Verify Embeddings"],"metadata":{"id":"oixLKV6uEsJz"}},{"cell_type":"code","source":["# Verify embeddings are valid and vary across tweets\n","print(\"=\"*60)\n","print(\"EMBEDDING VERIFICATION\")\n","print(\"=\"*60)\n","\n","# Stack all embeddings into a matrix\n","embedding_matrix = np.stack(df['embedding'].values)\n","print(f\"\\nEmbedding matrix shape: {embedding_matrix.shape}\")\n","\n","# Check for any zero vectors (would indicate failed extraction)\n","zero_vectors = np.sum(np.all(embedding_matrix == 0, axis=1))\n","print(f\"Zero vectors (failed extractions): {zero_vectors}\")\n","\n","# Check embedding statistics\n","print(f\"\\nEmbedding statistics:\")\n","print(f\"  Mean: {embedding_matrix.mean():.4f}\")\n","print(f\"  Std:  {embedding_matrix.std():.4f}\")\n","print(f\"  Min:  {embedding_matrix.min():.4f}\")\n","print(f\"  Max:  {embedding_matrix.max():.4f}\")\n","\n","# Check variance across embeddings (should be non-zero)\n","embedding_variance = np.var(embedding_matrix, axis=0).mean()\n","print(f\"  Mean variance across dimensions: {embedding_variance:.4f}\")\n","\n","# Quick semantic sanity check: similar texts should have similar embeddings\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","text1 = \"AI regulation is important for safety\"\n","text2 = \"Artificial intelligence rules matter for security\"\n","text3 = \"I had pizza for lunch today\"\n","\n","emb1 = get_embedding(text1, tokenizer, model, device).reshape(1, -1)\n","emb2 = get_embedding(text2, tokenizer, model, device).reshape(1, -1)\n","emb3 = get_embedding(text3, tokenizer, model, device).reshape(1, -1)\n","\n","sim_12 = cosine_similarity(emb1, emb2)[0][0]\n","sim_13 = cosine_similarity(emb1, emb3)[0][0]\n","\n","print(f\"\\nSemantic sanity check:\")\n","print(f\"  '{text1[:40]}...'\")\n","print(f\"  vs '{text2[:40]}...' → similarity: {sim_12:.3f}\")\n","print(f\"  vs '{text3[:40]}...' → similarity: {sim_13:.3f}\")\n","print(f\"  ✓ Similar texts more similar: {sim_12 > sim_13}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3cy1lhY2EtFo","executionInfo":{"status":"ok","timestamp":1765987771794,"user_tz":300,"elapsed":50,"user":{"displayName":"Darren Deng","userId":"14952694693563686285"}},"outputId":"8e74dda3-2a65-41f3-c3c8-bd42f238d9f8"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["============================================================\n","EMBEDDING VERIFICATION\n","============================================================\n","\n","Embedding matrix shape: (3201, 768)\n","Zero vectors (failed extractions): 0\n","\n","Embedding statistics:\n","  Mean: 0.0199\n","  Std:  0.4560\n","  Min:  -7.8555\n","  Max:  11.2109\n","  Mean variance across dimensions: 0.0045\n","\n","Semantic sanity check:\n","  'AI regulation is important for safety...'\n","  vs 'Artificial intelligence rules matter for...' → similarity: 0.989\n","  vs 'I had pizza for lunch today...' → similarity: 0.956\n","  ✓ Similar texts more similar: True\n"]}]},{"cell_type":"markdown","source":["# Cell 6: Save Embeddings"],"metadata":{"id":"YHr9qqvzE3jK"}},{"cell_type":"code","source":["# Save dataframe with embeddings as pickle\n","# (CSV can't properly store numpy arrays)\n","\n","output_path = os.path.join(PATHS['processed'], '02_tweets_with_embeddings.pkl')\n","\n","print(f\"Saving embeddings to: {output_path}\")\n","df.to_pickle(output_path)\n","\n","# Verify save\n","file_size = os.path.getsize(output_path) / 1e6\n","print(f\"✓ Saved successfully!\")\n","print(f\"  File size: {file_size:.1f} MB\")\n","\n","# Test reload\n","df_test = pd.read_pickle(output_path)\n","print(f\"  Reload test: {df_test['embedding'].iloc[0].shape} ✓\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jzrQBpbIE5kp","executionInfo":{"status":"ok","timestamp":1765987823618,"user_tz":300,"elapsed":102,"user":{"displayName":"Darren Deng","userId":"14952694693563686285"}},"outputId":"08c1ce41-570c-49e9-addf-e383367f3681"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Saving embeddings to: /content/drive/MyDrive/same_words_different_worlds/data/processed/02_tweets_with_embeddings.pkl\n","✓ Saved successfully!\n","  File size: 11.6 MB\n","  Reload test: (768,) ✓\n"]}]},{"cell_type":"markdown","source":["# Cell 7: Notebook 04 Summary"],"metadata":{"id":"phaao5hjFX-a"}},{"cell_type":"code","source":["print(\"=\"*60)\n","print(\"NOTEBOOK 04 COMPLETE ✓\")\n","print(\"=\"*60)\n","print(\"\"\"\n","EMBEDDING EXTRACTION RESULTS:\n","\n","1. PROCESS:\n","   - Loaded fine-tuned RoBERTa model\n","   - Applied mean pooling over token embeddings\n","   - Extracted 768-dimensional vector per tweet\n","\n","2. OUTPUT:\n","   - 3,201 tweets with embeddings\n","   - Matrix shape: (3201, 768)\n","   - Zero failed extractions\n","   - File: 02_tweets_with_embeddings.pkl (11.6 MB)\n","\n","3. VERIFICATION:\n","   - Semantic sanity check passed\n","   - Similar texts have higher cosine similarity\n","   - Embeddings show appropriate variance\n","\n","WHAT THESE EMBEDDINGS CAPTURE:\n","   - Each tweet is now a point in 768-dimensional space\n","   - Semantically similar tweets are closer together\n","   - We can now measure the \"distance\" between how\n","     Democrats and Republicans use the same words\n","\n","NEXT STEPS:\n","   → Notebook 05: Semantic Divergence Analysis\n","      - Visualize partisan semantic spaces (PCA/UMAP)\n","      - Train classifier probe\n","      - Measure word-level semantic distance\n","      - This is where we prove the thesis!\n","\"\"\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"awks3gKGFZDE","executionInfo":{"status":"ok","timestamp":1765987951485,"user_tz":300,"elapsed":5,"user":{"displayName":"Darren Deng","userId":"14952694693563686285"}},"outputId":"bb80e6c1-137b-4b6f-b5d9-3814616a28ba"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["============================================================\n","NOTEBOOK 04 COMPLETE ✓\n","============================================================\n","\n","EMBEDDING EXTRACTION RESULTS:\n","\n","1. PROCESS:\n","   - Loaded fine-tuned RoBERTa model\n","   - Applied mean pooling over token embeddings\n","   - Extracted 768-dimensional vector per tweet\n","   \n","2. OUTPUT:\n","   - 3,201 tweets with embeddings\n","   - Matrix shape: (3201, 768)\n","   - Zero failed extractions\n","   - File: 02_tweets_with_embeddings.pkl (11.6 MB)\n","   \n","3. VERIFICATION:\n","   - Semantic sanity check passed\n","   - Similar texts have higher cosine similarity\n","   - Embeddings show appropriate variance\n","\n","WHAT THESE EMBEDDINGS CAPTURE:\n","   - Each tweet is now a point in 768-dimensional space\n","   - Semantically similar tweets are closer together\n","   - We can now measure the \"distance\" between how\n","     Democrats and Republicans use the same words\n","   \n","NEXT STEPS:\n","   → Notebook 05: Semantic Divergence Analysis\n","      - Visualize partisan semantic spaces (PCA/UMAP)\n","      - Train classifier probe\n","      - Measure word-level semantic distance\n","      - This is where we prove the thesis!\n","\n"]}]}]}